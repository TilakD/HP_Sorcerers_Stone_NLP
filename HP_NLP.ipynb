{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Semantic similarity extraction using word vectors in Harry Potter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#future is the missing compatibility layer between Python 2 and Python 3. \n",
    "#It allows you to use a single, clean Python 3.x-compatible codebase to \n",
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#word encoding\n",
    "import codecs\n",
    "#finds all pathnames matching a pattern, like regex\n",
    "import glob\n",
    "#log events for libraries\n",
    "import logging\n",
    "#concurrency\n",
    "import multiprocessing\n",
    "#dealing with operating system\n",
    "import os\n",
    "#pretty print, human readable\n",
    "import pprint\n",
    "#regular expressions\n",
    "import re\n",
    "#natural language toolkit\n",
    "import nltk\n",
    "#word 2 vec   (conda install -c anaconda gensim=1.0.1)\n",
    "import gensim.models.word2vec as w2v\n",
    "#dimensionality reduction\n",
    "import sklearn.manifold\n",
    "#math\n",
    "import numpy as np\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#parse dataset\n",
    "import pandas as pd\n",
    "#visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK tokenizer models (only the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DTILAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DTILAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##stopwords like the at a an, unnecesasry\n",
    "##tokenization into sentences, punkt \n",
    "##http://www.nltk.org/\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Corpus\n",
    "### Load book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found book:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\1-18 books combined.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the book names, matching txt file\n",
    "book_filenames = sorted(glob.glob(\"..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\*.txt\"))\n",
    "print(\"Found book:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the book into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\1-18 books combined.txt'...\n",
      "Corpus is now 1704913 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize raw unicode , we'll add all text to this file in memory\n",
    "corpus_raw = u\"\"\n",
    "\n",
    "#for each book, read it, open it in utf 8 format, \n",
    "#add it to the raw corpus\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print (\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the corpus into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizastion\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#tokenize into sentences\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#remove unnnecessary, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Above all these qualities, he was a devoted servant of Lord Vishnu, and therefore he was given the title, \"King of kings\".\n",
      "[u'Above', u'all', u'these', u'qualities', u'he', u'was', u'a', u'devoted', u'servant', u'of', u'Lord', u'Vishnu', u'and', u'therefore', u'he', u'was', u'given', u'the', u'title', u'King', u'of', u'kings']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 293,443 tokens.\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens.\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "# more dimensions, more computationally expensive to train\n",
    "# but also more accurate\n",
    "# more dimensions = more generalized\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "# more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "# 0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the random number generator, to make the results reproducible.\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HP2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-08 13:38:40,088 : INFO : collecting all words and their counts\n",
      "2017-04-08 13:38:40,088 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-08 13:38:40,137 : INFO : PROGRESS: at sentence #10000, processed 170274 words, keeping 9385 word types\n",
      "2017-04-08 13:38:40,174 : INFO : collected 11426 word types from a corpus of 293443 raw words and 17714 sentences\n",
      "2017-04-08 13:38:40,177 : INFO : Loading a fresh vocabulary\n",
      "2017-04-08 13:38:40,194 : INFO : min_count=3 retains 5695 unique words (49% of original 11426, drops 5731)\n",
      "2017-04-08 13:38:40,197 : INFO : min_count=3 leaves 286106 word corpus (97% of original 293443, drops 7337)\n",
      "2017-04-08 13:38:40,217 : INFO : deleting the raw counts dictionary of 11426 items\n",
      "2017-04-08 13:38:40,220 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-04-08 13:38:40,220 : INFO : downsampling leaves estimated 213862 word corpus (74.7% of prior 286106)\n",
      "2017-04-08 13:38:40,221 : INFO : estimated required memory for 5695 words and 300 dimensions: 16515500 bytes\n",
      "2017-04-08 13:38:40,244 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "HP2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 5695\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(HP2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec training, this might take a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-08 13:39:17,424 : INFO : training model with 4 workers on 5695 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-04-08 13:39:17,424 : INFO : expecting 17714 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-08 13:39:18,575 : INFO : PROGRESS: at 15.51% examples, 148795 words/s, in_qsize 8, out_qsize 1\n",
      "2017-04-08 13:39:19,612 : INFO : PROGRESS: at 33.83% examples, 167295 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-08 13:39:20,609 : INFO : PROGRESS: at 50.85% examples, 171667 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-08 13:39:21,627 : INFO : PROGRESS: at 68.02% examples, 173313 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-08 13:39:22,617 : INFO : PROGRESS: at 84.42% examples, 173654 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-08 13:39:23,421 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-08 13:39:23,461 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-08 13:39:23,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-08 13:39:23,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-08 13:39:23,515 : INFO : training on 1467215 raw words (1069556 effective words) took 6.1s, 175837 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1069556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HP2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file, can be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-08 13:40:53,283 : INFO : saving Word2Vec object under trained\\HP2vec.w2v, separately None\n",
      "2017-04-08 13:40:53,283 : INFO : not storing attribute syn0norm\n",
      "2017-04-08 13:40:53,283 : INFO : not storing attribute cum_table\n",
      "2017-04-08 13:40:53,390 : INFO : saved trained\\HP2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "HP2vec.save(os.path.join(\"trained\", \"HP2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-08 13:41:10,157 : INFO : loading Word2Vec object from trained\\HP2vec.w2v\n",
      "2017-04-08 13:41:10,263 : INFO : loading wv recursively from trained\\HP2vec.w2v.wv.* with mmap=None\n",
      "2017-04-08 13:41:10,263 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-04-08 13:41:10,265 : INFO : setting ignored attribute cum_table to None\n",
      "2017-04-08 13:41:10,265 : INFO : loaded trained\\HP2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "HP2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"HP2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress the word vectors into 3D space using t-SNE and plot them for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=3,perplexity=15.0, n_iter=20000,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = HP2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train t-SNE, this could take few minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_3d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
